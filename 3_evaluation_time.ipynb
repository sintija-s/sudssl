{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from autorank import autorank, plot_stats, create_report\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, to_rgb\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": ['DejaVu Serif', 'Bitstream Vera Serif', 'Computer Modern Roman', 'New Century Schoolbook', 'Century Schoolbook L', 'Utopia', 'ITC Bookman', 'Bookman', 'Nimbus Roman No9 L', 'Palatino', 'Charter', 'serif'],\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"legend.title_fontsize\": 12,\n",
    "    \"legend.fontsize\": 11,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 11,\n",
    "    \"lines.linewidth\": 2,\n",
    "    \"lines.markersize\": 6,\n",
    "    \"legend.frameon\": False,\n",
    "    \"figure.figsize\": (9, 3)  # standard size for subplots\n",
    "})\n",
    "COLORS = ['#4c90b8', '#2ac3c1', '#f5b811', '#de653e', '#ff912a']\n",
    "color_dict = {\n",
    "    'BaselineAll': 'black',\n",
    "    'BaselineNone': 'gray',\n",
    "    'DisagreementSampling(DT)': COLORS[3],\n",
    "    'DisagreementSampling(DS)': COLORS[4],\n",
    "    'RandomSampling': COLORS[0],\n",
    "    # add other sampling methods as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Load Result Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downstream performance and time results\n",
    "df_all = pd.read_csv('results/tabnet.csv')\n",
    "\n",
    "df_all['sampling_method'] = df_all['sampling_method'].replace({\n",
    "    'ConsensusSampling(DT)': 'DisagreementSampling(DT)',\n",
    "    'ConsensusSampling(DS)': 'DisagreementSampling(DS)'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert sign for running times\n",
    "df_all.loc[df_all['metric'] == 'total_time', 'value'] *= -1\n",
    "df_all.loc[df_all['metric'] == 'sampling_time', 'value'] *= -1\n",
    "df_all.loc[df_all['metric'] == 'model_time', 'value'] *= -1\n",
    "\n",
    "# cleaner presentation of brackets\n",
    "df_clean = df_all.copy()\n",
    "bracket_cleaner = {40: '0-40', 60: '20-60', 80: '40-80', 100: '60-100', 0: 'all', np.nan: 'all'}\n",
    "df_clean['bracket'] = df_clean['bracket_max'].map(bracket_cleaner)\n",
    "df_clean = df_clean.drop(['bracket_min', 'bracket_max', 'mean', 'sampling_strategy', 'n_selected'], axis=1)\n",
    "\n",
    "# joining corresponding baselines\n",
    "baseline_none = df_clean[df_clean['sampling_method'] == 'BaselineNone'].drop(['bracket', 'sampling_method', 'n_sample'], axis=1).rename(columns={'value': 'baseline_none'})\n",
    "baseline_all = df_clean[df_clean['sampling_method'] == 'BaselineAll'].drop(['bracket', 'sampling_method', 'n_sample'], axis=1).rename(columns={'value': 'baseline_all'})\n",
    "df_clean = df_clean[~df_clean['sampling_method'].isin(['BaselineNone', 'BaselineAll'])]\n",
    "join_keys = ['dataset', 'n_labeled', 'metric', 'seed']\n",
    "df_clean = pd.merge(df_clean, baseline_none, on=join_keys, how='left')\n",
    "df_clean = pd.merge(df_clean, baseline_all, on=join_keys, how='left')\n",
    "df_clean_avg = df_clean.groupby(['dataset', 'n_labeled', 'n_sample', 'sampling_method', 'metric', 'bracket'])[['value', 'baseline_none', 'baseline_all']].mean().reset_index()\n",
    "\n",
    "# finding the best bracket by AUPRC\n",
    "df_agg = df_clean_avg[df_clean_avg['metric'] == 'auprc_macro']\n",
    "idx_max_value = df_agg.groupby(['dataset', 'n_labeled', 'sampling_method', 'n_sample'])['value'].idxmax()\n",
    "df_best_bracket = df_agg.loc[idx_max_value]\n",
    "idx_max_value = df_agg.groupby(['dataset', 'n_labeled', 'sampling_method'])['value'].idxmax()\n",
    "df_best_nsample_bracket = df_agg.loc[idx_max_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Time Compared to Baselines\n",
    "\n",
    "How much faster than the baselines is sampling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter to only AUPRC and total_time\n",
    "df_eff = df_clean_avg[df_clean_avg['metric'].isin(['auprc_macro', 'total_time'])].copy()\n",
    "df_eff = df_eff[df_eff['sampling_method'] != 'ConsensusSampling(DS)']  ### <----- or remove DT\n",
    "\n",
    "# Step 2: Pivot to wide format with value + baselines\n",
    "df_eff = df_eff.pivot_table(\n",
    "    index=['dataset', 'n_labeled', 'n_sample', 'sampling_method', 'bracket'],\n",
    "    columns='metric',\n",
    "    values=['value', 'baseline_all', 'baseline_none']\n",
    ").reset_index()\n",
    "\n",
    "# Flatten the multi-index columns\n",
    "df_eff.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col for col in df_eff.columns]\n",
    "\n",
    "# Step 3: Get best row per group (highest value_auprc_macro)\n",
    "df_best = df_eff.sort_values('value_auprc_macro', ascending=False).groupby(\n",
    "    ['dataset', 'n_labeled', 'n_sample', 'sampling_method'], as_index=False\n",
    ").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best['time_ratio_to_baseline_all'] = df_best['value_total_time'] / df_best['baseline_all_total_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_sample  sampling_method         \n",
       "100       DisagreementSampling(DS)    0.198008\n",
       "          DisagreementSampling(DT)    0.199271\n",
       "          RandomSampling              0.205311\n",
       "250       DisagreementSampling(DS)    0.218954\n",
       "          DisagreementSampling(DT)    0.218949\n",
       "          RandomSampling              0.215452\n",
       "500       DisagreementSampling(DS)    0.255799\n",
       "          DisagreementSampling(DT)    0.255688\n",
       "          RandomSampling              0.252686\n",
       "750       DisagreementSampling(DS)    0.295843\n",
       "          DisagreementSampling(DT)    0.296200\n",
       "          RandomSampling              0.293007\n",
       "1000      DisagreementSampling(DS)    0.329250\n",
       "          DisagreementSampling(DT)    0.329841\n",
       "          RandomSampling              0.331858\n",
       "Name: time_ratio_to_baseline_all, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time = df_best.groupby(['n_sample','sampling_method'])['time_ratio_to_baseline_all'].mean()\n",
    "df_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llr}\n",
      "\\toprule\n",
      " &  & time_ratio_to_baseline_all \\\\\n",
      "n_sample & sampling_method &  \\\\\n",
      "\\midrule\n",
      "\\multirow[t]{3}{*}{100} & DisagreementSampling(DS) & 0.198008 \\\\\n",
      " & DisagreementSampling(DT) & 0.199271 \\\\\n",
      " & RandomSampling & 0.205311 \\\\\n",
      "\\cline{1-3}\n",
      "\\multirow[t]{3}{*}{250} & DisagreementSampling(DS) & 0.218954 \\\\\n",
      " & DisagreementSampling(DT) & 0.218949 \\\\\n",
      " & RandomSampling & 0.215452 \\\\\n",
      "\\cline{1-3}\n",
      "\\multirow[t]{3}{*}{500} & DisagreementSampling(DS) & 0.255799 \\\\\n",
      " & DisagreementSampling(DT) & 0.255688 \\\\\n",
      " & RandomSampling & 0.252686 \\\\\n",
      "\\cline{1-3}\n",
      "\\multirow[t]{3}{*}{750} & DisagreementSampling(DS) & 0.295843 \\\\\n",
      " & DisagreementSampling(DT) & 0.296200 \\\\\n",
      " & RandomSampling & 0.293007 \\\\\n",
      "\\cline{1-3}\n",
      "\\multirow[t]{3}{*}{1000} & DisagreementSampling(DS) & 0.329250 \\\\\n",
      " & DisagreementSampling(DT) & 0.329841 \\\\\n",
      " & RandomSampling & 0.331858 \\\\\n",
      "\\cline{1-3}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_time.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Time vs. AUPRC\n",
    "\n",
    "Close look at the time/performance trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
