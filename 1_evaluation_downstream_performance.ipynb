{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from autorank import autorank, plot_stats, create_report\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, to_rgb\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "# from pymfe.mfe import MFE\n",
    "from dataset import load_data\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": ['DejaVu Serif', 'Bitstream Vera Serif', 'Computer Modern Roman', 'New Century Schoolbook', 'Century Schoolbook L', 'Utopia', 'ITC Bookman', 'Bookman', 'Nimbus Roman No9 L', 'Palatino', 'Charter', 'serif'],\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"legend.title_fontsize\": 12,\n",
    "    \"legend.fontsize\": 11,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 11,\n",
    "    \"lines.linewidth\": 2,\n",
    "    \"lines.markersize\": 6,\n",
    "    \"legend.frameon\": False,\n",
    "    \"figure.figsize\": (9, 3)  # standard size for subplots\n",
    "})\n",
    "COLORS = ['#4c90b8', '#2ac3c1', '#f5b811', '#de653e', '#ff912a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Load Results Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset metafeatures\n",
    "df_meta = pd.read_csv('results/metafeatures.csv')\n",
    "df_meta = df_meta[df_meta['seed'] == 'average']\n",
    "df_meta = df_meta.drop(columns=['seed'])\n",
    "df_meta = df_meta.loc[:, df_meta.nunique(dropna=False) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downstream performance and time results\n",
    "df_all = pd.read_csv('results/tabnet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert sign for running times\n",
    "df_all.loc[df_all['metric'] == 'total_time', 'value'] *= -1\n",
    "df_all.loc[df_all['metric'] == 'sampling_time', 'value'] *= -1\n",
    "df_all.loc[df_all['metric'] == 'model_time', 'value'] *= -1\n",
    "\n",
    "# cleaner presentation of brackets\n",
    "df_clean = df_all.copy()\n",
    "bracket_cleaner = {40: '0-40', 60: '20-60', 80: '40-80', 100: '60-100', 0: 'all', np.nan: 'all'}\n",
    "df_clean['bracket'] = df_clean['bracket_max'].map(bracket_cleaner)\n",
    "df_clean = df_clean.drop(['bracket_min', 'bracket_max', 'mean', 'sampling_strategy', 'n_selected'], axis=1)\n",
    "\n",
    "# joining corresponding baselines\n",
    "baseline_none = df_clean[df_clean['sampling_method'] == 'BaselineNone'].drop(['bracket', 'sampling_method', 'n_sample'], axis=1).rename(columns={'value': 'baseline_none'})\n",
    "baseline_all = df_clean[df_clean['sampling_method'] == 'BaselineAll'].drop(['bracket', 'sampling_method', 'n_sample'], axis=1).rename(columns={'value': 'baseline_all'})\n",
    "df_clean = df_clean[~df_clean['sampling_method'].isin(['BaselineNone', 'BaselineAll'])]\n",
    "join_keys = ['dataset', 'n_labeled', 'metric', 'seed']\n",
    "df_clean = pd.merge(df_clean, baseline_none, on=join_keys, how='left')\n",
    "df_clean = pd.merge(df_clean, baseline_all, on=join_keys, how='left')\n",
    "df_clean_avg = df_clean.groupby(['dataset', 'n_labeled', 'n_sample', 'sampling_method', 'metric', 'bracket'])[['value', 'baseline_none', 'baseline_all']].mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical metafeatures (low/medium/high)\n",
    "df_meta_cat = df_meta.copy()\n",
    "df_meta_cat = df_meta_cat.loc[:, df_meta_cat.nunique(dropna=False) > 2]\n",
    "feature_cols = [col for col in df_meta_cat.columns if col != \"dataset_id\"]\n",
    "\n",
    "# Apply quantile-based binning to each feature\n",
    "for col in feature_cols:\n",
    "    try:\n",
    "        df_meta_cat[col] = pd.qcut(df_meta_cat[col], q=3, labels=[\"low\", \"medium\", \"high\"])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sampling vs. All Unlabeled Data vs. None (Rank)\n",
    "\n",
    "Does sampling generally work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sampling vs. Baselines (AUPRC)\n",
    "\n",
    "How large are the gaps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. n_sample\n",
    "\n",
    "How much do we need to sample?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Brackets\n",
    "\n",
    "Which disagreement bracket makes the most sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Bracket Recommender\n",
    "\n",
    "We can get good results, but when should we use which bracket? Let's train a model on the dataset features and find out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Bracket Recommender: Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Bracket Recommender: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
